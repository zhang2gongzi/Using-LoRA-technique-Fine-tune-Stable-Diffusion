# Using-LoRA-technique-Fine-tune-Stable-Diffusion
使用LoRA微调文生图sd大模型
LoRA参数优化：

lora_rank从32提升到64：增加模型的表达能力和容量
lora_alpha从16提升到32：增加LoRA的权重，使模型更好地学习训练数据的特征
学习率调整：

基础学习率从1e-4降低到5e-5：较小的学习率有助于模型更稳定地学习
text_encoder_learning_rate设为UNet学习率的一半：这是推荐的最佳实践，可以帮助模型更好地理解概念
预热步数从100增加到200：更长的预热期有助于模型在早期阶段更稳定地学习
训练和验证设置优化：

max_train_steps从200增加到1000：增加训练步数以确保模型充分学习
validation_prompt_num从3增加到4：增加验证图像数量，获得更全面的模型评估
validation_step_ratio从1降低到0.2：降低验证频率但保持足够的监控，可以加快训练速度
这些修改的预期效果：

更强的模型表达能力（通过增加LoRA参数）
更稳定的训练过程（通过优化学习率和预热步数）
更充分的训练（通过增加训练步数）
更全面的验证评估（通过增加验证图像数量）
